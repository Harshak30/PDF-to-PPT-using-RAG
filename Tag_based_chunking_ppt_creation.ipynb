{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607aca34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "607aca34",
        "outputId": "cd01124f-c090-44a6-9ef0-e08fe6c4b02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.99)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain_community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain_community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain_community) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ebfc81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f7ebfc81",
        "outputId": "bb0a3db3-1bfb-4693-afce-8a3ac03cdd14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ced8abc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4ced8abc",
        "outputId": "fcf8de5a-8d53-4c7c-d4f2-f21f62de93de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LFle91uTebQh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LFle91uTebQh",
        "outputId": "09cb6f91-de4d-4166-9cb2-8a1f03ff9439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (9.4.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (3.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "pip install python-pptx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97467afb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "97467afb",
        "outputId": "00479a2f-3a40-4091-da37-2f98267f4704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: pinecone-client==5.0.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (5.0.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1->pinecone) (2024.7.4)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1->pinecone) (1.0.3)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1->pinecone) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1->pinecone) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1->pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==5.0.1->pinecone) (2.0.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PryV2E_FXbD7",
      "metadata": {
        "id": "PryV2E_FXbD7"
      },
      "outputs": [],
      "source": [
        "pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51fac1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c51fac1d",
        "outputId": "1bbf3fe0-f1df-4b3b-a23d-9e51d5700700"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "import json\n",
        "import requests\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pptx import Presentation\n",
        "from pptx.util import Pt\n",
        "from pptx.dml.color import RGBColor\n",
        "from pptx.util import Inches\n",
        "from pptx.enum.shapes import MSO_SHAPE\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "import os\n",
        "import fitz\n",
        "import math\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to match tags using LLM\n",
        "def match_tags(text, tags):\n",
        "    prompt = (\n",
        "        f\"Please read the following text and assign the most appropriate tag from the provided list and Ensure that the tags focus on the core concept of the text and do not include common terms or context that may pertain to the entire document. Also ignore the page related details while generating the tags:\\n\\n\"\n",
        "        f\"{text}\\n\\n\"\n",
        "        f\"{tags}\\n\\n\"\n",
        "        f\"First consider the tag list to match the most appropriate tags from the list incase if list not matching provide a tag. Provide only the tag without any additional text or explanation. Do not add sentences in beginning and formatted as follows:\\n\\n [Tag1,Tag2,Tag3]\"\n",
        "    )\n",
        "    payload = json.dumps({\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 2048,\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.95\n",
        "    })\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, data=payload, headers=headers)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "        response_json = response.json()\n",
        "\n",
        "        # Debug: Print the raw response\n",
        "        #print(\"Raw response:\", response_json)\n",
        "\n",
        "        # Extract the tags from the response\n",
        "        if isinstance(response_json, list) and 'text' in response_json[0]:\n",
        "            response_text = response_json[0]['text'].strip()\n",
        "            #print(f\"Response text: {response_text}\")\n",
        "            pattern = r'\\[(.*?)\\]'\n",
        "            if \"None\" in response_text:\n",
        "              #print(f\"No matching tags found: {response_text}\")\n",
        "              return [\"Unknown\"]\n",
        "            else:\n",
        "              match = re.search(pattern, response_text)\n",
        "              tags = match.group(1).split(',')\n",
        "              return [tag.strip() for tag in tags]\n",
        "\n",
        "        else:\n",
        "            #print(f\"Unexpected response format: {response_json}\")\n",
        "            return [\"Unknown\"]\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "        return [\"Unknown\"]\n",
        "\n",
        "def character_based_chunking(data):\n",
        "    # Split your data into smaller documents with chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=200)\n",
        "    documents = text_splitter.split_documents(data)\n",
        "    return documents\n",
        "\n",
        "def bytes_based_chunking(data):\n",
        "    # Extract text from each Document object and join into a single string\n",
        "    text_data = ''.join([doc.page_content for doc in data])\n",
        "    # Encode the text into bytes\n",
        "    text_bytes = text_data.encode('utf-8')\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text_bytes):\n",
        "        # Determine the end of the chunk\n",
        "        end = start + 2000\n",
        "        # Ensure not to split in the middle of a character\n",
        "        while end < len(text_bytes) and (text_bytes[end] & 0xC0) == 0x80:\n",
        "            end -= 1\n",
        "        # Decode the chunk back to a string and append to chunks list\n",
        "        chunks.append(text_bytes[start:end].decode('utf-8', errors='ignore'))\n",
        "        start = end\n",
        "    return chunks\n",
        "\n",
        "def page_based_chunking(data):\n",
        "    pages_text = [doc.page_content for doc in data]\n",
        "    return pages_text\n",
        "\n",
        "def extract_font_sizes(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    font_sizes = set()\n",
        "\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc[page_num]\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "        for block in blocks:\n",
        "            if \"lines\" in block:\n",
        "                for line in block[\"lines\"]:\n",
        "                    for span in line[\"spans\"]:\n",
        "                        font_size = abs(span[\"size\"])\n",
        "                        font_sizes.add(font_size)  # Add the font size to the set\n",
        "    sorted_font_sizes = sorted(font_sizes, reverse=True)\n",
        "    print(sorted_font_sizes)\n",
        "    if len(sorted_font_sizes) == 0:\n",
        "        return None  # No font sizes found\n",
        "\n",
        "    if len(sorted_font_sizes) == 1:\n",
        "        return sorted_font_sizes[0]  # Only one font size, return it as the largest\n",
        "\n",
        "    return sorted_font_sizes[1]\n",
        "\n",
        "def extract_content_below_headings(pdf_path, size_threshold):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    chunks = []\n",
        "    current_heading = None\n",
        "    current_content = \"\"\n",
        "\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc[page_num]\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "        for block in blocks:\n",
        "            if \"lines\" in block:\n",
        "                for line in block[\"lines\"]:\n",
        "                    for span in line[\"spans\"]:\n",
        "                        font_size = span[\"size\"]\n",
        "                        text = span[\"text\"].strip()\n",
        "\n",
        "                        if not text:\n",
        "                            continue\n",
        "\n",
        "                        # Check if the text is a heading based on the font size\n",
        "                        if font_size > size_threshold:\n",
        "                            # Save the current content under the previous heading\n",
        "                            if current_heading and current_content.strip():\n",
        "                                chunks.append({\"heading\": current_heading, \"content\": current_content.strip()})\n",
        "                            # Start a new section with the new heading\n",
        "                            current_heading = text\n",
        "                            current_content = \"\"\n",
        "                        else:\n",
        "                            # Append text to the current section's content\n",
        "                            current_content += \" \" + text\n",
        "\n",
        "    # Append the last section if it has content\n",
        "    if current_heading and current_content.strip():\n",
        "        chunks.append({\"heading\": current_heading, \"content\": current_content.strip()})\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_chunks(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    data = loader.load()\n",
        "    ctype= int(input(\"Select the chunk type by number: 1) character based chunking 2) Byte size based chunking 3) Pagewise chunking 4) Section wise chunking( based on font size)\"))\n",
        "    if ctype == 1:\n",
        "      chunks=character_based_chunking(data)\n",
        "    elif ctype == 2:\n",
        "      chunks=bytes_based_chunking(data)\n",
        "    elif ctype == 3:\n",
        "      chunks=page_based_chunking(data)\n",
        "    elif ctype == 4:\n",
        "      font_sizes = extract_font_sizes(pdf_path)\n",
        "      print(\"Font sizes found in the PDF:\", math.floor(font_sizes))\n",
        "      chunks = extract_content_below_headings(pdf_path,math.floor(font_sizes)-1)\n",
        "\n",
        "    # Convert Document objects into strings\n",
        "    texts = [str(doc) for doc in chunks]\n",
        "    print(\"Chunk Size :\",len(chunks))\n",
        "    return texts\n",
        "\n",
        "def create_tags(texts):\n",
        "    tags = ['Introduction', 'Subheadings','Subheadings','Conclusion'] # Provide titles and subtitles that are required\n",
        "    document_tags = []\n",
        "    for text in texts:\n",
        "        tag = match_tags(text, tags)\n",
        "        document_tags.append({\n",
        "            'Document': text,\n",
        "            'Tag': tag\n",
        "        })\n",
        "    document_tags_df = pd.DataFrame(document_tags)\n",
        "    return document_tags_df\n",
        "\n",
        "def generate_embeddings(text):\n",
        "    return model.encode(text).tolist()\n",
        "\n",
        "def add_chunks_to_pinecone(document_tags_df):\n",
        "  # Store only the tags' embeddings in Pinecone and print the vectors\n",
        "  for i, row in document_tags_df.iterrows():\n",
        "      tags_embedding = generate_embeddings(row['Document'])\n",
        "      index.upsert([\n",
        "          {\n",
        "              'id': f'tags-{i}',\n",
        "              'values': tags_embedding,\n",
        "              'metadata': {'text': row['Document']}\n",
        "          }\n",
        "      ], namespace=f\"{row['Tag']}\")\n",
        "      #print(f\"Vector for tags-{i}: {tags_embedding}\")\n",
        "\n",
        "  print(\"Tags' embeddings stored in Pinecone successfully.\")\n",
        "\n",
        "\n",
        "# Function to get tags using LLaMA 3 model\n",
        "def get_tags(prompt):\n",
        "    url = \"llama38b\"\n",
        "    payload = json.dumps({\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 2048,\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.95\n",
        "    })\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    try:\n",
        "        response = requests.post(url, data=payload, headers=headers)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "        response_json = response.json()\n",
        "\n",
        "        # Extract the tags from the response\n",
        "        if isinstance(response_json, list) and 'text' in response_json[0]:\n",
        "            response_text = response_json[0]['text'].strip()\n",
        "            #print(f\"Response text: {response_text}\")  # Debugging line\n",
        "            return response_text\n",
        "        else:\n",
        "            #print(f\"Unexpected response format: {response_json}\")\n",
        "            return [\"Unknown\"]\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "        return [\"Unknown\"]\n",
        "\n",
        "def extract_keywords(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [word for word in tokens if word.isalnum()]\n",
        "\n",
        "# Compare keywords with tags and find relevant chunks across multiple namespaces\n",
        "def find_relevant_chunks(question):\n",
        "    keywords = extract_keywords(question)\n",
        "    #print(f\"Extracted keywords: {keywords}\")\n",
        "    query_embeddings = generate_embeddings(' '.join(keywords))\n",
        "    #print(f\"Query embeddings: {query_embeddings[:5]}...\")\n",
        "\n",
        "    response = index.describe_index_stats()\n",
        "    namespaces = list(response['namespaces'].keys())\n",
        "    #print(f\"Namespaces: {namespaces}\")\n",
        "\n",
        "    all_matches = []\n",
        "\n",
        "    # Filter namespaces based on keywords\n",
        "    filtered_namespaces = [ns for ns in namespaces if any(kw.lower() in ns.lower() for kw in keywords)]\n",
        "    #print(f\"Filtered Namespaces: {filtered_namespaces}\")\n",
        "\n",
        "    # Query each filtered namespace\n",
        "    for namespace in filtered_namespaces:\n",
        "        response = index.query(\n",
        "            vector=query_embeddings,\n",
        "            top_k=5,\n",
        "            include_metadata=True,\n",
        "            namespace=namespace\n",
        "        )\n",
        "\n",
        "        #print(f\"Response from namespace {namespace}: {response}\")\n",
        "\n",
        "        matches = response.get('matches', [])\n",
        "        all_matches.extend(matches)\n",
        "\n",
        "    all_matches = sorted(all_matches, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    return all_matches\n",
        "\n",
        "# Function to generate the final summary using LLM\n",
        "def create_summary_from_relevant_chunks(relevant_chunks):\n",
        "    concatenated_text = \"\\n\".join([chunk['metadata']['text'] for chunk in relevant_chunks])\n",
        "\n",
        "    # Create a prompt for the LLM to generate a summary\n",
        "    summary_prompt = f\"Summarize the following information into a concise summary:\\n\\n{concatenated_text}\\n\\nSummary:\"\n",
        "    summary = get_tags(summary_prompt)\n",
        "    return summary\n",
        "\n",
        "# Function to load PowerPoint templates from a directory\n",
        "def load_templates(directory):\n",
        "    templates = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".pptx\"):\n",
        "            templates[filename] = os.path.join(directory, filename)\n",
        "    return templates\n",
        "\n",
        "# Function to select a PowerPoint template interactively\n",
        "def select_template(templates):\n",
        "    print(\"Available templates:\")\n",
        "    for i, template in enumerate(templates.keys(), 1):\n",
        "        print(f\"{i}. {template}\")\n",
        "    choice = int(input(\"Select a template by number: \")) - 1\n",
        "    selected_template = list(templates.values())[choice]\n",
        "    return selected_template\n",
        "\n",
        "def add_slide(prs, title, content, word_limit=80):\n",
        "    words = content.split()\n",
        "    while len(words) > word_limit:\n",
        "        # Get the portion of the content that fits within the word limit\n",
        "        current_content = ' '.join(words[:word_limit])\n",
        "        # Add the slide with the current portion of the content\n",
        "        create_slide(prs, title, current_content)\n",
        "        # Update the words list to contain the remaining words\n",
        "        words = words[word_limit:]\n",
        "        # Update the title to indicate continuation\n",
        "        title = \"to be contd..\"\n",
        "    # Add the final slide with the remaining content\n",
        "    create_slide(prs, title, ' '.join(words))\n",
        "\n",
        "def create_slide(prs, title, content):\n",
        "    slide_layout = prs.slide_layouts[1]  # Use the layout that has a title and content\n",
        "    slide = prs.slides.add_slide(slide_layout)\n",
        "    title_placeholder = slide.shapes.title\n",
        "    content_placeholder = slide.placeholders[1]\n",
        "\n",
        "    # Set title\n",
        "    title_placeholder.text = title\n",
        "    for paragraph in title_placeholder.text_frame.paragraphs:\n",
        "        for run in paragraph.runs:\n",
        "            run.font.size = Pt(24)  # Set title font size\n",
        "            run.font.bold = True  # Set title font bold\n",
        "            run.font.name = 'Arial'  # Set title font style\n",
        "\n",
        "    # Split content into bullet points using '•', '.', or '*'\n",
        "    bullet_points = re.split(r'[•*]', content)\n",
        "    bullet_points = [bp.strip() for bp in bullet_points if bp.strip()]  # Clean and remove empty entries\n",
        "\n",
        "    # Clear any existing content\n",
        "    text_frame = content_placeholder.text_frame\n",
        "    text_frame.clear()\n",
        "\n",
        "    # Add bullet points\n",
        "    for point in bullet_points:\n",
        "        p = text_frame.add_paragraph()\n",
        "        p.text = point\n",
        "        p.level = 0  # Level 0 for top-level bullets\n",
        "        p.font.size = Pt(16)\n",
        "        p.font.name = 'Calibri'\n",
        "        p.font.color.rgb = RGBColor(0, 0, 0)  # Black colo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea581792",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea581792",
        "outputId": "cf4c5a26-3e98-49b1-fe2b-cf19fde7e2b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Select the chunk type by number: 1) character based chunking 2) Byte size based chunking 3) Pagewise chunking 4) Section wise chunking1\n",
            "Chunk Size : 30\n",
            "Tags' embeddings stored in Pinecone successfully.\n",
            "Available templates:\n",
            "1. template1.pptx\n",
            "2. template2.pptx\n",
            "3. template4.pptx\n",
            "4. template3.pptx\n",
            "Select a template by number: 2\n",
            "Enter the title (type 'done' to finish): Projection for the export\n",
            "Enter the title (type 'done' to finish): Capex\n",
            "Enter the title (type 'done' to finish): Price Discounting\n",
            "Enter the title (type 'done' to finish): Consolidating Margin\n",
            "Enter the title (type 'done' to finish): done\n",
            "Presentation saved to output_presentation.pptx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "url = \"llama38b\"\n",
        "pdf_path = r\"Sample.pdf\" # Any pdf \n",
        "texts=create_chunks(pdf_path)\n",
        "document_tags_df=create_tags(texts)\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize Pinecone\n",
        "pinecone = Pinecone(api_key='') # include api_key\n",
        "dimension = model.get_sentence_embedding_dimension()  # Dimension for 'all-MiniLM-L6-v2' model\n",
        "pinecone.delete_index(\"chunks\")\n",
        "pinecone.create_index(\n",
        "      name=\"chunks\",\n",
        "      dimension=dimension,\n",
        "      metric=\"cosine\",\n",
        "      spec=ServerlessSpec(\n",
        "          cloud=\"aws\",\n",
        "          region=\"us-east-1\"\n",
        "      )\n",
        "  )\n",
        "index = pinecone.Index(\"chunks\")\n",
        "add_chunks_to_pinecone(document_tags_df)\n",
        "\n",
        "#PPT\n",
        "templates_directory = r\"/content/templates\"\n",
        "templates = load_templates(templates_directory)\n",
        "selected_template_path = select_template(templates)\n",
        "\n",
        "prs = Presentation(selected_template_path)\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"Enter the title (type 'done' to finish): \")\n",
        "    if user_question.lower() == \"done\":\n",
        "        break\n",
        "    relevant_chunks = find_relevant_chunks(user_question)\n",
        "    summary = create_summary_from_relevant_chunks(relevant_chunks)\n",
        "    add_slide(prs, user_question, summary)\n",
        "\n",
        "output_path = r\"output_presentation.pptx\"\n",
        "prs.save(output_path)\n",
        "print(f\"Presentation saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZdLLZlyNig9F",
      "metadata": {
        "id": "ZdLLZlyNig9F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
